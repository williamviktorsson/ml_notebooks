{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation - Nätverket som lär sig självt\n",
    "\n",
    "I förra notebooken byggde vi ett XOR-nätverk med manuellt inställda vikter. Det fungerade, men det är opraktiskt för riktiga problem med tusentals neuroner.\n",
    "\n",
    "Nu ska vi lära oss **Backpropagation** - algoritmen som låter neurala nätverk hitta sina egna vikter genom att propagera felet bakåt genom nätverket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "När nätverket gissar fel uppstår en **total kostnad** (felet). Denna kostnad är beroende av alla vikter i hela nätverket tillsammans. Ändrar vi en enda vikt litegrann, så kommer den totala kostnaden att förändras. Målet är att justera nätverket så att vi närmar oss en kostnad på 0 (inget fel!).\n",
    "\n",
    "Kostnaden kan alltså ses som en funktion av vikterna! **Gradient Descent** är strategin vi använder för att hitta den kombination av vikter som ger lägst möjliga kostnad.\n",
    "\n",
    "Strategin är super-simpel! För varje enskild vikt behöver vi bara räkna ut:\n",
    "- Åt vilket håll påverkar den den totala kostnaden?\n",
    "- Hur mycket?\n",
    "\n",
    "Denna lutning kallas för **gradienten**.\n",
    "\n",
    "### Intuition: Gå nedför backen\n",
    "\n",
    "Tänk dig att du står på en kulle i dimma och vill hitta dalen (lägsta kostnaden). Du kan inte se var dalen är, men du kan känna vilken riktning det lutar under dina fötter. Strategin är enkel:\n",
    "\n",
    "1. Känn efter vilken riktning det lutar (beräkna gradienten)\n",
    "2. Ta ett steg i den riktningen (uppdatera vikten)\n",
    "3. Upprepa tills du når botten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uppdateringsregeln\n",
    "\n",
    "Vår uppdateringsregel för varje enskild vikt blir:\n",
    "\n",
    "```\n",
    "ny_vikt = gammal_vikt - α × gradienten_för_denna_vikt\n",
    "```\n",
    "\n",
    "Här är `α` (alpha) vår **inlärningsfaktor** som bestämmer hur stora steg vi tar.\n",
    "\n",
    "- **Hög gradient (brant lutning):** Vi har mycket att vinna på att ändra vikten. Ta större steg!\n",
    "- **Låg gradient (flack lutning):** Vi är nära botten. Ta mindre steg för att finjustera.\n",
    "\n",
    "Men hur beräknar vi gradienten för varje vikt, speciellt för vikter i gömda lager? Det är här **Backpropagation** kommer in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation är en algoritm för att beräkna gradienten för varje vikt. Den gör detta genom att först beräkna ett **\"felansvar\"** (delta, δ) för varje enskild neuron.\n",
    "\n",
    "Olika neuroner kommer alltså vara olika ansvariga för eventuella fel i nätverket. Givet ett fel i slutet, behöver vi hitta vilka som var mest bidragande till felet, och korrigera deras vikter mest!\n",
    "\n",
    "### De fyra stegen\n",
    "\n",
    "Processen för ett enskilt tränings-exempel sker alltid i fyra steg:\n",
    "\n",
    "**Steg 1: Framåtpasset (Forward Pass)**\n",
    "Skicka in data och låt den flöda framåt genom nätverket. **Spara alla outputs** från varje neuron - vi behöver dem senare!\n",
    "\n",
    "**Steg 2: Delta för Output-lagret**\n",
    "För varje neuron i sista lagret:\n",
    "```\n",
    "δ_output = (facit - gissning) × derivatan_av_sigmoid(gissning)\n",
    "```\n",
    "Varför derivatan? Jo! En hög derivata(lutning) för gissningen betyder att en justering av dess vikter har större påverkan på utfallet. Derivatan är dessutom högre när neuronen är \"mindre säker\" alltså desto närmre ett värde på 0.5 som gissningen närmar sig. Vi kan tolka det som att vi ger ett högre delta till de neuroner som \"inte riktigt har bestämt sig\" eller \"de som påverkas mest av att justeras\".\n",
    "\n",
    "**Steg 3: Delta för Dolda Lager (baklänges)**\n",
    "För varje neuron i ett dolt lager:\n",
    "```\n",
    "δ_dold = (summerat fel från nästa lager) × derivatan_av_sigmoid(output)\n",
    "```\n",
    "\n",
    "**Steg 4: Uppdatera Vikter**\n",
    "```\n",
    "ny_vikt = gammal_vikt + α × δ × input_som_passerade_vikten\n",
    "ny_bias = gammal_bias + α × δ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid-derivatan\n",
    "\n",
    "En smart egenskap hos sigmoid-funktionen är att dess derivata är enkel att beräkna:\n",
    "\n",
    "Om `s = sigmoid(x)`, då är `sigmoid'(x) = s × (1 - s)`\n",
    "\n",
    "Vi behöver alltså inte beräkna derivatan från scratch - vi kan använda outputen direkt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)  # För reproducerbarhet\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x < -700:\n",
    "        return 0.0\n",
    "    if x > 700:\n",
    "        return 1.0\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sig_output):\n",
    "    \"\"\"Derivatan av sigmoid, givet sigmoid-outputen.\"\"\"\n",
    "    return sig_output * (1 - sig_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Övning 7: XOR med gömda lager SOM LÄR SIG\n",
    "\n",
    "I denna övning ska du implementera backpropagation för ett simpelt XOR-nätverk med ett gömt lager.\n",
    "\n",
    "Nätverket har:\n",
    "- 2 inputs\n",
    "- 2 neuroner i det gömda lagret\n",
    "- 1 output-neuron\n",
    "\n",
    "**Din uppgift:** Fyll i TODO:erna för att implementera backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weights = [random.uniform(-1, 1) for _ in range(num_inputs)]\n",
    "        self.bias = random.uniform(-1, 1)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        total = sum(i * w for i, w in zip(inputs, self.weights)) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "\n",
    "class XORNetwork:\n",
    "    def __init__(self):\n",
    "        # Skapa neuronerna explicit\n",
    "        self.hidden1 = Neuron(num_inputs=2)\n",
    "        self.hidden2 = Neuron(num_inputs=2)\n",
    "        self.output_neuron = Neuron(num_inputs=2)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Framåtmatning\n",
    "        h1_out = self.hidden1.predict(inputs)\n",
    "        h2_out = self.hidden2.predict(inputs)\n",
    "        final_out = self.output_neuron.predict([h1_out, h2_out])\n",
    "        return final_out\n",
    "\n",
    "    def train(self, inputs_list, targets, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            for x, target in zip(inputs_list, targets):\n",
    "                # === FORWARD PASS ===\n",
    "                h1_out = self.hidden1.predict(x)\n",
    "                h2_out = self.hidden2.predict(x)\n",
    "                final_out = self.output_neuron.predict([h1_out, h2_out])\n",
    "\n",
    "                # TODO 1: Beräkna felet i sista lagret\n",
    "                # error = facit - output\n",
    "                output_error = target - final_out\n",
    "\n",
    "                # TODO 2: Beräkna delta för output-neuronen\n",
    "                # delta = error × sigmoid_derivative(output)\n",
    "                output_delta = output_error * sigmoid_derivative(final_out)\n",
    "\n",
    "                # TODO 3: Beräkna felet för varje hidden neuron\n",
    "                # Error = delta_nästa × vikt_som_kopplar_dem\n",
    "                h1_error = output_delta * self.output_neuron.weights[0]\n",
    "                h2_error = output_delta * self.output_neuron.weights[1]\n",
    "\n",
    "                # TODO 4: Beräkna delta för varje hidden neuron\n",
    "                # delta = error × sigmoid_derivative(output)\n",
    "                h1_delta = h1_error * sigmoid_derivative(h1_out)\n",
    "                h2_delta = h2_error * sigmoid_derivative(h2_out)\n",
    "\n",
    "                # TODO 5: Uppdatera vikter för output-neuronen\n",
    "                # vikt += learning_rate × delta × input_till_vikten\n",
    "                self.output_neuron.weights[0] += learning_rate * output_delta * h1_out\n",
    "                self.output_neuron.weights[1] += learning_rate * output_delta * h2_out\n",
    "                self.output_neuron.bias += learning_rate * output_delta\n",
    "\n",
    "                # TODO 6: Uppdatera vikter för hidden neuronerna\n",
    "                self.hidden1.weights[0] += learning_rate * h1_delta * x[0]\n",
    "                self.hidden1.weights[1] += learning_rate * h1_delta * x[1]\n",
    "                self.hidden1.bias += learning_rate * h1_delta\n",
    "\n",
    "                self.hidden2.weights[0] += learning_rate * h2_delta * x[0]\n",
    "                self.hidden2.weights[1] += learning_rate * h2_delta * x[1]\n",
    "                self.hidden2.bias += learning_rate * h2_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tränar XOR-nätverket...\n",
      "\n",
      "--- Resultat ---\n",
      "Input: [0, 0], Facit: 0, Prediktion: 0.0171 (≈0) ✓\n",
      "Input: [0, 1], Facit: 1, Prediktion: 0.9805 (≈1) ✓\n",
      "Input: [1, 0], Facit: 1, Prediktion: 0.9841 (≈1) ✓\n",
      "Input: [1, 1], Facit: 0, Prediktion: 0.0152 (≈0) ✓\n"
     ]
    }
   ],
   "source": [
    "# Träna och testa\n",
    "xor_inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "xor_targets = [0, 1, 1, 0]\n",
    "\n",
    "network = XORNetwork()\n",
    "\n",
    "print(\"Tränar XOR-nätverket...\")\n",
    "network.train(xor_inputs, xor_targets, epochs=10000, learning_rate=0.5)\n",
    "\n",
    "print(\"\\n--- Resultat ---\")\n",
    "for x, t in zip(xor_inputs, xor_targets):\n",
    "    pred = network.predict(x)\n",
    "    rounded = round(pred)\n",
    "    status = \"✓\" if rounded == t else \"✗\"\n",
    "    print(f\"Input: {x}, Facit: {t}, Prediktion: {pred:.4f} (≈{rounded}) {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fantastiskt!** Om du implementerat TODO:erna korrekt ska nätverket nu kunna lösa XOR - något som var omöjligt med en enda perceptron!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Övning 8: Ett generellt neuralt nätverk\n",
    "\n",
    "Nu ska du bygga ett fullt kapabelt neuralt nätverk som kan ha godtyckligt antal inputs, outputs och gömda lager.\n",
    "\n",
    "Öppna `neural_network.py` och granska koden. Den innehåller en halvfärdig implementation av `NeuralNetwork`-klassen med backpropagation. Du har några TODO:s där som du borde kunna lösa när du förstått backpropagation.\n",
    "\n",
    "Kör sedan exemplet nedan för att testa ditt nätverk!\n",
    "\n",
    "Arkitekturen definieras med `layer_sizes`. Till exempel:\n",
    "- `[2, 2, 1]` = 2 inputs, 2 dolda neuroner, 1 output (XOR)\n",
    "- `[64, 30, 10]` = 64 inputs, 30 dolda, 10 outputs (siffror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "\n",
    "# Testa på XOR\n",
    "X_train = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_train = [[0], [1], [1], [0]]\n",
    "\n",
    "nn = NeuralNetwork(layer_sizes=[2, 2, 1])\n",
    "\n",
    "print(\"Startar träning av neuralt nätverk för XOR...\")\n",
    "nn.train(X_train, y_train, epochs=10000, learning_rate=0.5)\n",
    "\n",
    "print(\"\\nTestresultat:\")\n",
    "for inputs, target in zip(X_train, y_train):\n",
    "    prediction = nn.predict(inputs)\n",
    "    rounded = round(prediction[0])\n",
    "    status = \"✓\" if rounded == target[0] else \"✗\"\n",
    "    print(f\"Input: {inputs} -> Gissning: {prediction[0]:.4f} (≈{rounded}), Facit: {target[0]} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Övning 9: Modifiera ett Neuralt Nätverk\n",
    "\n",
    "**Syfte:** Att förstå hur arkitekturen i ett neuralt nätverk påverkar dess förmåga att lära sig.\n",
    "\n",
    "### Experiment 1: Flaskhals - Färre Neuroner\n",
    "\n",
    "Ändra arkitekturen från `[2, 2, 1]` till `[2, 1, 1]`. Du har nu skapat en \"flaskhals\" med bara en enda neuron i det dolda lagret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Flaskhals\n",
    "nn_bottleneck = NeuralNetwork(layer_sizes=[2, 1, 1])\n",
    "\n",
    "print(\"Tränar nätverk med FLASKHALS [2, 1, 1]...\")\n",
    "nn_bottleneck.train(X_train, y_train, epochs=10000, learning_rate=0.5)\n",
    "\n",
    "print(\"\\nResultat med flaskhals:\")\n",
    "correct = 0\n",
    "for inputs, target in zip(X_train, y_train):\n",
    "    prediction = nn_bottleneck.predict(inputs)\n",
    "    rounded = round(prediction[0])\n",
    "    if rounded == target[0]:\n",
    "        correct += 1\n",
    "    status = \"✓\" if rounded == target[0] else \"✗\"\n",
    "    print(f\"Input: {inputs} -> {prediction[0]:.4f} (≈{rounded}), Facit: {target[0]} {status}\")\n",
    "\n",
    "print(f\"\\nNoggrannhet: {correct}/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fråga:** Kan nätverket fortfarande lösa XOR? Varför/varför inte?\n",
    "\n",
    "> *Hint: En enda neuron i det gömda lagret kan bara skapa en rak linje. XOR kräver minst två linjer för att separera datan.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Djupare Nätverk\n",
    "\n",
    "Ändra arkitekturen till `[2, 4, 4, 1]`. Du har nu skapat ett djupare nätverk med två dolda lager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Djupare nätverk\n",
    "nn_deep = NeuralNetwork(layer_sizes=[2, 4, 4, 1])\n",
    "\n",
    "print(\"Tränar DJUPT nätverk [2, 4, 4, 1]...\")\n",
    "nn_deep.train(X_train, y_train, epochs=10000, learning_rate=0.5)\n",
    "\n",
    "print(\"\\nResultat med djupt nätverk:\")\n",
    "correct = 0\n",
    "for inputs, target in zip(X_train, y_train):\n",
    "    prediction = nn_deep.predict(inputs)\n",
    "    rounded = round(prediction[0])\n",
    "    if rounded == target[0]:\n",
    "        correct += 1\n",
    "    status = \"✓\" if rounded == target[0] else \"✗\"\n",
    "    print(f\"Input: {inputs} -> {prediction[0]:.4f} (≈{rounded}), Facit: {target[0]} {status}\")\n",
    "\n",
    "print(f\"\\nNoggrannhet: {correct}/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fråga:** Lär sig det djupare nätverket snabbare eller långsammare? Blir resultatet mer stabilt?\n",
    "\n",
    "> **Reflektera:** Du har nu agerat som en AI-arkitekt. Hur påverkade antalet neuroner och lager modellens prestanda? Detta är kärnan i **hyperparameter-tuning**: att hitta den bästa arkitekturen och de bästa inställningarna för ett givet problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Jämför olika arkitekturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "architectures = [\n",
    "    [2, 2, 1],\n",
    "    [2, 3, 1],\n",
    "    [2, 4, 1],\n",
    "    [2, 2, 2, 1],\n",
    "    [2, 4, 4, 1],\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    nn = NeuralNetwork(layer_sizes=arch)\n",
    "    nn.train(X_train, y_train, epochs=5000, learning_rate=0.5)\n",
    "    \n",
    "    # Beräkna MSE\n",
    "    mse = 0\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        pred = nn.predict(x)\n",
    "        mse += (y[0] - pred[0]) ** 2\n",
    "    mse /= len(X_train)\n",
    "    \n",
    "    results.append((str(arch), mse))\n",
    "    print(f\"Arkitektur {arch}: MSE = {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vad har vi lärt oss?\n",
    "\n",
    "I denna notebook har vi:\n",
    "\n",
    "1. Förstått **Gradient Descent** - att gå nedför kostnadslandskapet\n",
    "2. Lärt oss **Backpropagation** - att propagera felet bakåt för att hitta varje vikts bidrag\n",
    "3. Implementerat backprop för ett **XOR-nätverk** med gömda lager\n",
    "4. Byggt ett **generellt `NeuralNetwork`** med godtycklig arkitektur\n",
    "5. Experimenterat med olika arkitekturer och sett hur de påverkar lärandet\n",
    "\n",
    "## Nästa steg\n",
    "\n",
    "I nästa notebook använder vi vårt färdiga nätverk för att lösa riktiga problem:\n",
    "- Handskrivna siffror\n",
    "- Regression (huspriser)\n",
    "- Text (Bag of Words)\n",
    "- Bilder (Autoencoder)\n",
    "\n",
    "Vi kommer också lära oss om de två stora familjerna av maskininlärning: **Supervised** och **Unsupervised Learning**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
