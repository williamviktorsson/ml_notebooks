<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>deltas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="deltas_files/libs/clipboard/clipboard.min.js"></script>
<script src="deltas_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="deltas_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="deltas_files/libs/quarto-html/popper.min.js"></script>
<script src="deltas_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="deltas_files/libs/quarto-html/anchor.min.js"></script>
<link href="deltas_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="deltas_files/libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="deltas_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="deltas_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="deltas_files/libs/bootstrap/bootstrap-81267100e462c21b3d6c0d5bf76a3417.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="loss" class="level1">
<h1>Loss</h1>
<p>Hittills har vi pratat främst om att vi kan beräkna en error efter att nätverket har gjort sina uppskattningar.</p>
<p>Ett nätverk som har 4 outputs kanske har facit [0,0,1,0] och uppskattningar [0.1, 0.2, 0.6, 0.1] och därför kan vi beskriva deras error som [0.1, 0.2, -0.4, 0.1].</p>
<p>Men låt säga att vi vill ha ett bra mått på hur fel nätverket har i snitt? Ska vi summera felen? Ska vi ta det största felet? Ska vi ta det minsta felet? Ska vi ta det genomsnittliga felet?</p>
<p>Låt säga att vi tar genomsnittet av felen, så i det här fallet skulle det vara <span class="math inline">\((0.1 + 0.2 - 0.4 + 0.1) / 4 = 0.0\)</span>. Det är inte så bra..</p>
<p>Så vi vill representera felen på ett sätt som inte tar ut varandra. Samtidigt vill vi att större fel ska ge ett större straff, så att nätverket kan lära sig att undvika stora fel.</p>
<p>Det är här loss-funktionen kommer in. En funktion som tar in nätverkets uppskattningar och det faktiska facit, och ger oss en siffra som representerar hur fel nätverket är.</p>
<p>En vanlig loss-funktion är Mean Squared Error (MSE), som beräknas som genomsnittet av kvadraten av felen. I vårt exempel skulle det vara:</p>
<p><span class="math display">\[\text{MSE} = \frac{(0.1)^2 + (0.2)^2 + (-0.4)^2 + (0.1)^2}{4} = \frac{0.01 + 0.04 + 0.16 + 0.01}{4} = 0.055\]</span></p>
<p>Låt oss införa lite notation: vi kallar nätverkets output (uppskattning) för <span class="math inline">\(\hat{y}\)</span> (“y-hat”) och det faktiska facit (target) för <span class="math inline">\(y\)</span>.</p>
<p>En annan vanlig loss-funktion är Cross-Entropy Loss, som används för klassificeringsproblem. Den beräknas som <span class="math inline">\(-\sum y \cdot \log(\hat{y})\)</span>.</p>
<p>Dagens uppgift handlar om att först acceptera att det ni lärt er hittills är att skapa ett nätverk som använder MSE. Snart kommer ni förstå hur MSE används i det vi gjort hittills.</p>
<p>MSE per neuron kan vi beskriva såhär:</p>
<p><span class="math display">\[\text{loss}(\hat{y}) = (y - \hat{y})^2\]</span></p>
<hr>
<p>Vi tar först ett steg tillbaka. Vi har ett nätverk som tar in en input som är en lista av numeriska värden, och ger oss en output som också är en lista av numeriska värden. Vi har också ett <strong>“target”</strong> som är en lista av numeriska värden, som är det vi vill att nätverket ska ge oss.</p>
<p>Nätverket har en massa vikter som det använder för att beräkna sin output.</p>
<p>Det första lagret i nätverket tar in inputen och multiplicerar den med vikterna för det lagret, och ger oss en output. Det andra lagret tar den outputen och multiplicerar den med sina vikter, och så vidare tills vi får den slutliga outputen.</p>
<p>Nu vill vi att nätverket ska lära sig att ge oss en output som är så nära target som möjligt. För att göra det behöver vi en funktion som kan berätta för oss hur fel nätverket är, så att vi kan justera vikterna för att minska det felet.</p>
<p><strong>DET ENDA VI KAN GÖRA OCH BRYR OSS OM ATT GÖRA I DET LÄGET ÄR ATT JUSTERA VIKTERNA FÖR ATT MINSKA FELET.</strong></p>
<p>Så för varje vikt <span class="math inline">\(w_i\)</span> är vi intresserad av att beräkna <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span> (där <span class="math inline">\(L\)</span> är vår loss-funktion), det vill säga hur mycket loss förändras när vi ändrar den vikten. <strong>DET ÄR ALLT!!</strong> Det är det enda vi bryr oss om. Det är det enda som är relevant för att justera vikterna. Att hitta lutningen av loss-funktionen i förhållande till varje vikt, och sedan justera vikterna i riktning mot den lutningen för att minska loss. Detta kallas för <em>gradient descent</em> och det är grunden för hur neurala nätverk lär sig.</p>
<p>En gång till alltså: Vi behöver beräkna <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span> för varje vikt <span class="math inline">\(w_i\)</span> i nätverket, så att vi kan justera vikterna i riktning mot den lutningen för att minska loss. Det är det enda som är relevant för att justera vikterna. Det är det enda som är relevant för att lära sig. Det är just nu det enda som är relevant för att förstå hur neurala nätverk fungerar.</p>
<p>Okej.</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i}\]</span></p>
<p>Hur plockar vi fram den här informationen? Hur beräknar vi <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span>?</p>
<p>De funktioner vi har är:</p>
<p><strong>1. Loss-funktionen</strong> — hur fel är vi? (<span class="math inline">\(L\)</span> = loss, <span class="math inline">\(\hat{y}\)</span> = output, <span class="math inline">\(y\)</span> = target)</p>
<p><span class="math display">\[L(\hat{y}) = (y - \hat{y})^2\]</span></p>
<p><strong>2. Aktiveringsfunktionen</strong> — sigmoid (<span class="math inline">\(\sigma\)</span>) omvandlar den viktade summan <span class="math inline">\(s\)</span> till en output mellan 0 och 1:</p>
<p><span class="math display">\[\hat{y}(s) = \sigma(s) = \frac{1}{1 + e^{-s}}\]</span></p>
<p><strong>3. Den viktade summan</strong> — <span class="math inline">\(x_i\)</span> är input nummer <span class="math inline">\(i\)</span>, <span class="math inline">\(w_i\)</span> är vikten som hör till den inputen, och <span class="math inline">\(b\)</span> är bias:</p>
<p><span class="math display">\[s = \sum_i x_i \cdot w_i + b = x_1 \cdot w_1 + x_2 \cdot w_2 + \ldots + x_n \cdot w_n + b\]</span></p>
<p>Men vi vill ju beräkna derivatan med avseende på <em>en specifik</em> vikt <span class="math inline">\(w_i\)</span>. Ur <span class="math inline">\(w_i\)</span>:s perspektiv är alla andra termer konstanta, så vi kan skriva:</p>
<p><span class="math display">\[s(w_i) = x_i \cdot w_i + C\]</span></p>
<p>där <span class="math inline">\(C\)</span> är allt annat (alla andra vikter gånger deras inputs, plus bias).</p>
<p>Så om vi vill beskriva vår loss som en funktion av <span class="math inline">\(w_i\)</span>, så kan vi skriva:</p>
<p><span class="math display">\[L(w_i) = (y - \hat{y}(s(w_i)))^2\]</span></p>
<p><strong>BOOM!</strong> Nu har vi loss som en funktion av <span class="math inline">\(w_i\)</span>, och vi kan beräkna <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span> genom att använda kedjeregeln för derivering. <strong>WOW!!!</strong></p>
<p>Snabb påminnelse om kedjeregeln: Om vi har en funktion <span class="math inline">\(f(g(x))\)</span>, så är derivatan <span class="math inline">\(f'(g(x)) \cdot g'(x)\)</span>. Det vill säga, vi deriverar den yttre funktionen och multiplicerar den med derivatan av den inre funktionen.</p>
<p>Och om vi har en sammansatt funktion i tre steg. t.ex. <span class="math inline">\(f(g(h(x)))\)</span>, så är derivatan <span class="math inline">\(f'(g(h(x))) \cdot g'(h(x)) \cdot h'(x)\)</span>. Det vill säga, vi deriverar den yttre funktionen och multiplicerar den med derivatan av den mellersta funktionen och multiplicerar den med derivatan av den innersta funktionen.</p>
<p>Så vad behöver vi för att komma igång? Låt oss plocka fram derivatorna för varje funktion:</p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{y}} = -2(y - \hat{y})\]</span></p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial s} = \sigma(s) \cdot (1 - \sigma(s))\]</span></p>
<p>och eftersom <span class="math inline">\(\hat{y} = \sigma(s)\)</span>, och vi redan har <span class="math inline">\(\hat{y}\)</span> sparat, kan vi skriva:</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial s} = \hat{y} \cdot (1 - \hat{y})\]</span></p>
<p><span class="math display">\[\frac{\partial s}{\partial w_i} = x_i\]</span></p>
<p>Nu kan vi använda kedjeregeln för att beräkna <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span>:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial s} \cdot \frac{\partial s}{\partial w_i}\]</span></p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i} = -2(y - \hat{y}) \cdot \hat{y}(1 - \hat{y}) \cdot x_i\]</span></p>
<p>och notera, att under träningens gång, brukar vi spara på oss en neurons output och t.ex. spara den i en variabel som vi kallar för <span class="math inline">\(\hat{y}_n\)</span> (output från neuron <span class="math inline">\(n\)</span>), så att vi inte behöver beräkna <span class="math inline">\(\hat{y}(s)\)</span> varje gång vi vill beräkna <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span>, utan istället kan använda <span class="math inline">\(\hat{y}_n\)</span> direkt i formeln:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i} = -2(y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_i\]</span></p>
<p>Sen när vi har <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span>, så kan vi justera vikten <span class="math inline">\(w_i\)</span> i riktning mot den lutningen för att minska loss och då brukar vi använda en inlärningshastighet, som är en liten positiv konstant som vi multiplicerar med <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span> för att bestämma hur mycket vi ska justera vikten <span class="math inline">\(w_i\)</span>. Så den nya vikten <span class="math inline">\(w_i'\)</span> skulle vara:</p>
<p><span class="math display">\[w_i' = w_i + \eta \cdot \frac{\partial L}{\partial w_i}\]</span></p>
<p>vilket är detsamma som:</p>
<p><span class="math display">\[w_i' = w_i + \eta \cdot \big(-2(y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_i\big)\]</span></p>
<p>och den där siffran 2 kan vi också inkludera i inlärningshastigheten, så att vi inte behöver ha den där 2:an i formeln varje gång, utan istället kan ha en inlärningshastighet som är dubbelt så stor, och då skulle formeln för att justera vikten <span class="math inline">\(w_i\)</span> vara:</p>
<p><span class="math display">\[w_i' = w_i + \eta \cdot (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_i\]</span></p>
<hr>
<section id="delta---varför-skapar-vi-den" class="level2">
<h2 class="anchored" data-anchor-id="delta---varför-skapar-vi-den">Delta - varför skapar vi den?</h2>
<p>Okej, nu har vi en fungerande formel. Men titta på den en gång till. Säg att neuron <span class="math inline">\(n\)</span> har 5 vikter. Då behöver vi beräkna:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_1} = (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_1\]</span> <span class="math display">\[\frac{\partial L}{\partial w_2} = (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_2\]</span> <span class="math display">\[\frac{\partial L}{\partial w_3} = (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_3\]</span> <span class="math display">\[\frac{\partial L}{\partial w_4} = (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_4\]</span> <span class="math display">\[\frac{\partial L}{\partial w_5} = (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n) \cdot x_5\]</span></p>
<p>Ser du det? Allt utom sista termen är <strong>exakt samma</strong> varje gång. Det enda som skiljer sig är vilken input vi multiplicerar med.</p>
<p>Så vi sparar den gemensamma delen i en variabel och kallar den <strong>delta</strong> (<span class="math inline">\(\delta\)</span>):</p>
<p><span class="math display">\[\delta_n = (y - \hat{y}_n) \cdot \hat{y}_n(1 - \hat{y}_n)\]</span></p>
<p>Detta är deltan för neuron <span class="math inline">\(n\)</span>. Det är ett mått på hur fel neuronen har multiplicerat med “hur benägen den är att ändra sin output” (sigmoidens derivata). För sigmoid funktionen är derivatan störst när output är 0.5, och minskar när output närmar sig 0 eller 1. Det är alltså inte direkt ett “mått på hur osäker neuronen är”, utan snarare ett mått på hur mycket vi kan förändra neuronens fel genom att justera dess vikter, bias eller förbättra dess input. Men, det är nära besläktat med osäkerhet när vi använder sigmoid som aktiveringsfunktion. De neuroner som är mest osäkra (output nära 0.5) har störst potential att minska sitt fel genom att justera sina vikter. Så de neuroner med störst fel och som är mest osäkra kommer att ha störst delta-värde. Men det är inte osäkerhet vi mäter, det är en kombination av fel och benägenhet att ändra output. Det är precis som människor! Vissa människor är väldigt säkra på sina felaktiga åsikter och är därför svåra att övertyga att ändra sig, medan andra är mer öppna för nya idéer smiley face. Men förutom “öppenhet” så är ju människor olika benägna att lyssna på olika personer också. Vissa lyssnar de inte på alls. Vissa vill de bara göra tvärtom mot vad de säger. Vi kan se det som att personer har olika “vikter” för vilka personer de lyssnar på. Så snart pratar vi lite om neuroner, precis som människor, behöver ta hänsyn till inte bara fel, benägenhet att ändra sig, utan också vilka “vikter” de har för olika inputs, för att avgöra hur de kan påverka personer i sin omgivning, jag menar neuroner i sitt nätverk glad gubbe.</p>
<p>Ursäkta mitt sidetrack.. <strong>Nu blir viktuppdateringen mycket enklare:</strong></p>
<p><span class="math display">\[w_i' = w_i + \eta \cdot \delta_n \cdot x_i\]</span></p>
<p>Vi beräknar <span class="math inline">\(\delta_n\)</span> en gång per neuron, och sedan multiplicerar vi med varje input för att få varje vikts uppdatering. Perfekt! Vi har en matematiskt snygg och korrent lösning för att justera vikterna i output-lagret.</p>
<p>Men vänta lite.. Hur gör vi med de gömda lagren? De har inget target, så hur beräknar vi deras deltas?</p>
<hr>
</section>
<section id="delta-i-gömda-lager" class="level2">
<h2 class="anchored" data-anchor-id="delta-i-gömda-lager">Delta i gömda lager</h2>
<p>Formeln ovan fungerar för output-lagret, för där har vi ett target att jämföra med. Men vad gör vi med en neuron i ett gömt lager? Den har inget eget target. Den har inget facit.</p>
<p>Det den gömda neuronen däremot vet är att den skickar sin output vidare till neuroner i nästa lager. Och vi har precis beräknat delta för varje neuron i nästa lager. Varje sådant delta säger: “om mina förutsättningar hade varit lite bättre åt det här hållet, hade loss minskat.”</p>
<p>Det den gömda neuronen behöver lista ut är: åt vilket håll ska <em>jag</em> justeras för att nätverket totalt sett ska bli bättre?</p>
<p>Och det är en direkt beräkning. Vi har egentligen bara ett val att göra. Vår output påverkar neuronerna i nästa lager, och vi vet redan vad de behöver (deras deltas). Vi vet också hur starkt vi är kopplade till var och en av dem (vikterna). Så vi beräknar helt enkelt netto-effekten:</p>
<pre><code>      gömd neuron H
        |       |        |
      w=0.8   w=0.1    w=0.5
        |       |        |
      neuron A  neuron B  neuron C
      δ=0.3    δ=0.0     δ=0.7</code></pre>
<p>Neuron A säger “jag behöver hjälp åt det här hållet” (<span class="math inline">\(\delta_A = 0.3\)</span>), och H är starkt kopplad till A (vikt 0.8). Neuron C säger samma sak fast ännu starkare (<span class="math inline">\(\delta_C = 0.7\)</span>), och H är kopplad med vikt 0.5. Neuron B behöver ingen hjälp alls (<span class="math inline">\(\delta_B = 0.0\)</span>).</p>
<p>Netto-signalen som H tar emot:</p>
<p><span class="math display">\[0.8 \cdot 0.3 + 0.1 \cdot 0.0 + 0.5 \cdot 0.7 = 0.24 + 0 + 0.35 = 0.59\]</span></p>
<p>Det här talet säger åt neuronen H: “om du justerar din output åt det här hållet, så förbättras nätverket totalt sett med ungefär så här mycket.” Det är allt. Det är den enda informationen H har att gå på. Det är den enda informationen H behöver för att veta åt vilket håll den ska justera sig.</p>
<p>Okej, men kan vi uttrycka detta matematiskt? Vi vill ju beräkna <span class="math inline">\(\frac{\partial L}{\partial w_i}\)</span> för varje vikt i den gömda neuronen, precis som vi gjorde för output-lagret. Vi behöver kedjeregeln igen.</p>
<p>Kommer du ihåg att för output-lagret hade vi tre funktioner, och att vi kunde derivera var och en?</p>
<p><span class="math display">\[L(\hat{y}) = (y - \hat{y})^2 \qquad \hat{y}(s) = \sigma(s) \qquad s(w_i) = x_i \cdot w_i + C\]</span></p>
<p>För det gömda lagret har vi inget target, och därmed ingen enkel loss-funktion vi kan skriva ner. Men vi behöver egentligen inte det heller. Vi behöver bara <span class="math inline">\(\frac{\partial L}{\partial \hat{y}_H}\)</span> - alltså hur mycket nätverkets totala loss förändras om vi ändrar H:s output.</p>
<p>Och det kan vi redan räkna ut - med saker vi redan vet!</p>
<p>Kom ihåg vad vi visade för output-lagret:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i} = \delta \cdot x_i\]</span></p>
<p>Det säger: “effekten av att ändra vikt <span class="math inline">\(w_i\)</span> på loss är <span class="math inline">\(\delta\)</span> gånger inputen som hör till den vikten.”</p>
<p>Men tänk nu - vad <em>är</em> <span class="math inline">\(x_i\)</span> till en neuron i nästa lager? Det är <span class="math inline">\(\hat{y}_H\)</span>! Och vad är <span class="math inline">\(w_i\)</span>? Det är vikten <span class="math inline">\(w_k\)</span> som kopplar H till neuron <span class="math inline">\(k\)</span>.</p>
<p>Så vi kan vända på det. Från neuron <span class="math inline">\(k\)</span>:s perspektiv, om vi ändrar dess input (alltså H:s output), hur mycket påverkas loss? Samma logik:</p>
<p><span class="math display">\[\frac{\partial L}{\partial x_k} = \delta_k \cdot w_k\]</span></p>
<p>Och H matar in i <em>alla</em> neuroner i nästa lager. Så den totala effekten av att ändra H:s output är summan:</p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{y}_H} = \sum_k \delta_k \cdot w_k\]</span></p>
<p>Där har vi det. Nu har vi <span class="math inline">\(\frac{\partial L}{\partial \hat{y}_H}\)</span>, och resten av kedjan är identisk med output-lagret:</p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{y}_H} = \sum_k \delta_k \cdot w_k\]</span></p>
<p><span class="math display">\[\frac{\partial \hat{y}_H}{\partial s} = \hat{y}_H(1 - \hat{y}_H)\]</span></p>
<p><span class="math display">\[\frac{\partial s}{\partial w_i} = x_i\]</span></p>
<p>Kedjeregeln:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial \hat{y}_H} \cdot \frac{\partial \hat{y}_H}{\partial s} \cdot \frac{\partial s}{\partial w_i}\]</span></p>
<p><span class="math display">\[\frac{\partial L}{\partial w_i} = \left(\sum_k \delta_k \cdot w_k\right) \cdot \hat{y}_H(1 - \hat{y}_H) \cdot x_i\]</span></p>
<p>Och precis som förut plockar vi ut den gemensamma delen som delta:</p>
<p><span class="math display">\[\delta_H = \left(\sum_k \delta_k \cdot w_k\right) \cdot \hat{y}_H(1 - \hat{y}_H)\]</span></p>
<p>Jämför med output-lagrets delta:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Felsignal</th>
<th><span class="math inline">\(\times\)</span> sigmoidens derivata</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Output-lager:</strong></td>
<td><span class="math inline">\(\delta = \mathbf{(y - \hat{y})}\)</span></td>
<td><span class="math inline">\(\cdot\ \hat{y}(1 - \hat{y})\)</span></td>
</tr>
<tr class="even">
<td><strong>Gömt lager:</strong></td>
<td><span class="math inline">\(\delta_H = \mathbf{\sum_k \delta_k \cdot w_k}\)</span></td>
<td><span class="math inline">\(\cdot\ \hat{y}_H(1 - \hat{y}_H)\)</span></td>
</tr>
</tbody>
</table>
<p>Samma struktur. Det enda som skiljer sig är var “felsignalen” kommer ifrån. I output-lagret kommer den från target. I det gömda lagret kommer den från nästa lagers deltas, via kedjeregeln.</p>
<p>Och notera: vi behövde aldrig skriva loss som en funktion av <span class="math inline">\(\hat{y}_H\)</span>. Vi behövde bara kedjeregeln och det faktum att vi redan beräknat delta för nästa lager. Det är just det som gör backpropagation effektivt - vi återanvänder beräkningar vi redan gjort.</p>
<p>Viktuppdateringen är sedan som vanligt:</p>
<p><span class="math display">\[w_i' = w_i + \eta \cdot \delta_H \cdot x_i\]</span></p>
<p>Och <span class="math inline">\(\delta_H\)</span> skickas vidare bakåt till lagret innan, så att de neuronerna kan göra exakt samma beräkning.</p>
<p>Det är hela backpropagation:</p>
<ol type="1">
<li>Beräkna <span class="math inline">\(\delta\)</span> för output-lagret (vi har target)</li>
<li>Skicka <span class="math inline">\(\delta\)</span> bakåt och beräkna <span class="math inline">\(\delta\)</span> för varje gömt lager (via <span class="math inline">\(\sum \delta \cdot w\)</span>)</li>
<li>Uppdatera varje vikt: <span class="math inline">\(w_i' = w_i + \eta \cdot \delta \cdot x_i\)</span></li>
</ol>
<p>Samma formel, samma princip, lager för lager bakåt genom nätverket.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>