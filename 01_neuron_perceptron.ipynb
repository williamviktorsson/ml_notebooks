{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Den symboliska AI:n gav oss en stor uppsättning fantastiska verktyg som är relevanta än idag. Men förhoppningar om att nästa stora genombrott inom AI skulle komma från den inriktningen är sedan länge borta.\n",
    "\n",
    "När intresset för den symboliska AI:n avtog fanns det istället utrymme för en annan idé att växa fram. En idé som faktiskt föddes _innan_ termen \"Artificiell Intelligens\" ens fanns. Det var idén om att inte programmera intelligens, utan att _träna_ den. Att inte bygga logik, utan att bygga en hjärna som kan lära sig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectionism\n",
    "\n",
    "Redan 1943, långt innan AI-fältet fick sitt namn, hade Warren McCulloch och Walter Pitts skapat en matematisk modell av en biologisk neuron. 1958 byggde Frank Rosenblatt **Perceptronen**, en maskin som kunde lära sig känna igen former. New York Times skrev att detta var _embryot till en dator som kommer att kunna gå, prata, se, skriva, reproducera sig själv och vara medveten om sin existens._[^1]\n",
    "\n",
    "[^1]: https://www.nytimes.com/1958/07/13/archives/electronic-brain-teaches-itself.html\n",
    "\n",
    "1969 publicerades dock boken _Perceptrons_ av Marvin Minsky och Seymour Papert. I den bevisade de matematiskt att dessa tidiga nätverk hade fundamentala begränsningar (som vi ska se). Finansieringen ströps, forskarna bytte spår, och connectionismen fick klicka på snooze-knappen i bakgrunden i årtionden, medan den symboliska AI:n tog över scenen.\n",
    "\n",
    "Men när expertsystemen misslyckades, började forskare damma av de gamla idéerna igen. De insåg att felet inte låg i idén om neuroner, utan i att vi hade haft för få av dem och för lite data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronen\n",
    "\n",
    "Nervceller, som också kallas **neuroner** är en del av nervsystemet vars uppgift är att via signaler styra och koordinera kroppens olika funktioner. Vi behöver dem för att kunna reagera på information som skickas från våra sinnen och reagera på dem. Vi behöver dem för att kunna tänka och minnas saker. Över 80 miljarder neuroner uppskattas finnas i en mänsklig hjärna.\n",
    "\n",
    "Översiktligt består nervcellen av en **cellkropp** och två olika typer av utskott som är högst relevanta för oss: **dendriter**, som tar emot signaler, och en **axon**, som skickar vidare signaler. Slutet av axonen förgrenar sig i flera axonterminaler så att signalen kan skickas vidare i massa olika riktningar.\n",
    "\n",
    "**Synapser** kallas den struktur som bildas mellan neuroners axoner och andra neuroners dendriter. Synapsen överför signalerna som kommer via axonen, nervtråden, till receptorer på dendriten. Signalen som kommer via axonen är elektrisk och kan antingen skickas vidare som en elektrisk impuls eller omvandlas till en kemisk signal och skickas via neurotransmittorer.\n",
    "\n",
    "När flera neuroner är sammankopplade kallar vi dem tillsammans ett **neuralt nätverk**.\n",
    "\n",
    "En **avgörande egenskap** är vår hjärnas **plasticitet**, dess förmåga att skapa nya förbindelser eller på annat sätt ändra sin struktur och funktion. Detta sker som svar på yttre och inre påverkningar och tillåter oss att lära oss nya saker, kompensera för skador, anpassa oss till nya förutsättningar och bilda minner.\n",
    "\n",
    "Connectionism representerar idén om att vi kanske kan skapa artificiell intelligens genom att bygga system som behandlar och överför information på liknande sätt. Med små sammankopplade, ofta simpla enheter, som tillsammans bildar något oerhört kapabelt.\n",
    "\n",
    "![Översikt av neuronens komponenter](./images/neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Den artificiella neuronen\n",
    "\n",
    "Tänk dig en neuron som en enkel beslutsmodell. Dess jobb är att väga samman olika informationskällor (inputs) för att fatta ett ja/nej-beslut (output). Låt oss använda en konkret, tvådimensionell analogi: **Ska jag gå på festen?**\n",
    "\n",
    "För att fatta detta beslut har du två grundläggande frågor:\n",
    "\n",
    "1. **x1: Är vänner där?** (`1` för Ja, `0` för Nej)\n",
    "2. **x2: Har jag ett prov imorgon?** (`1` för Ja, `0` för Nej)\n",
    "\n",
    "Denna artificiella neuronmodell representerar resonemanget med två justerbara \"rattar\" för varje fråga, kallade **vikter** (`w1`, `w2`), samt en tredje, oberoende ratt kallad **bias** (`b`).\n",
    "\n",
    "- **Vikterna** representerar hur viktig varje faktor är. En hög positiv vikt för `w1` betyder \"det är superviktigt att mina vänner är där\". En stor negativ vikt för `w2` betyder \"ett prov är en stark anledning att stanna hemma\". **Geometriskt bestämmer vikterna lutningen, eller orienteringen, på neuronens beslutsgräns.**\n",
    "  - Det här sista om geometri kanske lät lite klurigt, vi kommer till det snart, du kommer se att ekvationen vi snart kommer skapa kommer bilda en linje genom ett koordinatsystem som skiljer alla möjliga kombinationer som leder till att du går på festen, från de kombinationer som leder till att du inte går på festen.\n",
    "- **Bias** representerar neuronens grundinställning. Den fungerar som en **justerbar tröskel** som representerar din grundläggande vilja att gå på fest (positiv bias) eller stanna hemma (negativ bias). Den är alltså **partisk** mot ett visst beslut.\n",
    "  - **Geometriskt bestämmer biasen positionen på beslutsgränsen; den \"knuffar\" gränsen bort från origo (nollpunkten), vilket ger den friheten att placeras var som helst.**\n",
    "\n",
    "Modellens beräkning är enkel: `Summa = (x1*w1) + (x2*w2) + b`.\n",
    "\n",
    "Geometriskt sett är detta mer än bara en summa; ekvationen `Summa = 0` definierar en perfekt **rak skiljelinje** i ett tvådimensionellt rum. Allt på ena sidan linjen är 'Ja', och allt på andra sidan är 'Nej'. När du sätter Summa = 0 kan du lätt skriva om uttrycket som en simpel `y = kx + m` och rita din raka skiljelinje!\n",
    "\n",
    "Beslutet fattas sedan av en **aktiveringsfunktion**: om `Summa > 0`, blir output `1` (Gå!). Annars blir den `0` (Stanna hemma!). Aktiveringsfunktionen avgör alltså på vilken sida om skiljelinjen din punkt befinner sig på.\n",
    "\n",
    "![Gå på festen?](./images/neuron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En Neuron i Python\n",
    "\n",
    "Innan vi lär en neuron att lära sig själv, låt oss först bygga själva mekanismen i en Python-klass. Denna `Neuron`-klass kommer att ha en `predict`-metod som utför beräkningen. Notera att den inte kan lära sig; istället kommer **vi** att manuellt ställa in dess vikter och bias för att skapa olika beslutsmodeller.\n",
    "\n",
    "Öppna filen `neuron.py` och fixa koden. Kör sedan cellen nedan för att testa den."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den Studiemotiverade: Input [1, 1] -> Beslut: 0 (Förväntat: 0)\n"
     ]
    }
   ],
   "source": [
    "# Importera vår Neuron-klass\n",
    "from neuron import Neuron\n",
    "\n",
    "# Persona 1: Den Studiemotiverade\n",
    "# Denna person prioriterar studier över allt annat.\n",
    "# Vänner är trevligt, men ett prov är en deal-breaker.\n",
    "studious_neuron = Neuron(weights=[0.5, -1.0], bias=-0.2)\n",
    "\n",
    "# Scenario: Vänner är där (1), men det är ett prov imorgon (1)\n",
    "inputs = [1, 1]\n",
    "decision = studious_neuron.predict(inputs)\n",
    "print(f\"Den Studiemotiverade: Input {inputs} -> Beslut: {decision} (Förväntat: 0)\")\n",
    "# Beräkning: (1*0.5) + (1*-1.0) + (-0.2) = -0.7. Resultat: 0 (Stanna hemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sociala Festprissen: Input [1, 1] -> Beslut: 1 (Förväntat: 1)\n"
     ]
    }
   ],
   "source": [
    "# Persona 2: Sociala Festprissen\n",
    "# Denna person älskar att umgås och har en mycket mer avslappnad inställning till prov.\n",
    "social_neuron = Neuron(weights=[1.0, -0.1], bias=0.5)\n",
    "\n",
    "# Samma scenario: Vänner är där (1), prov imorgon (1)\n",
    "inputs = [1, 1]\n",
    "decision = social_neuron.predict(inputs)\n",
    "print(f\"Sociala Festprissen: Input {inputs} -> Beslut: {decision} (Förväntat: 1)\")\n",
    "# Beräkning: (1*1.0) + (1*-0.1) + 0.5 = 1.4. Resultat: 1 (Gå på fest!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Detta visar tydligt hur vikter och bias direkt formar neuronens beslutsfattande. Klassen `Neuron` och dess `predict`-metod representerar den grundläggande beräkningsmekanismen.\n",
    "\n",
    "Men vad händer om vi har 100 inputs istället för 2? Att manuellt hitta de bästa värdena för 100 vikter och en bias blir en omöjlig uppgift. Vi behöver ett sätt för neuronen att _hitta de bästa parametrarna själv_ genom att titta på exempel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptronen\n",
    "\n",
    "På 50-talet tog Frank Rosenblatt neuronmodellen och gav den en enkel **läranderegel**. Kombinationen av neuronmodellen och denna regel är vad som kallas **Perceptronen**. Regeln låter den lära sig från sina misstag genom en trestegsprocess.\n",
    "\n",
    "Lärandet behöver en uppsättning **träningsdata**, vilket innehållet exempel på inputs med motsvarande output - alltså facit.\n",
    "\n",
    "### Perceptronens lärande\n",
    "\n",
    "För varje exempel i träningsdatan, upprepar Perceptronen följande tre steg:\n",
    "\n",
    "1. **Gissa:** Den tar emot inputs (`x1`, `x2`, ...) och beräknar en output (`0` eller `1`) med hjälp av sina nuvarande vikter och bias.\n",
    "2. **Beräkna Felet:** Den jämför sin gissning med det korrekta svaret (facit). Felet beräknas enkelt: `Fel = facit - gissning`. Detta kan bara resultera i tre möjliga värden:\n",
    "   - `0`: Gissningen var korrekt.\n",
    "   - `1`: Gissningen var `0` men borde ha varit `1`.\n",
    "   - `-1`: Gissningen var `1` men borde ha varit `0`.\n",
    "3. **Justera Parametrarna (Lärdom):** Om felet var `0` görs ingenting. Om felet var `1` eller `-1`, justeras alla parametrar för att göra gissningen lite bättre nästa gång. Justeringen följer en enkel formel:\n",
    "\n",
    "   - `ny_bias = gammal_bias + α * fel`\n",
    "   - `ny_vikt_x = gammal_vikt_x + α * fel * input_x`\n",
    "\n",
    "\n",
    "Här är `α` (alfa) en **inlärningsfaktor** (ofta ett litet tal som 0.1), som styr hur stora steg lärandet ska ta.\n",
    "\n",
    "Genom att upprepa dessa tre steg för många exempel, kommer neuronens vikter och bias gradvis att närma sig värden som löser problemet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exempel: Perceptronens lärande steg för steg\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario 1:**\n",
    "Du står inför ett beslut. Vännerna är på festen (`x1=1`) och du har ett prov imorgon (`x2=1`). Det korrekta beslutet för dig är att stanna hemma och plugga (facit = `0`).\n",
    "\n",
    "Din Perceptron har precis startat och har slumpmässiga startvärden:\n",
    "- `w1` (vänner) = `0.5`\n",
    "- `w2` (prov) = `-0.4`\n",
    "- `b` (bias) = `0.0`\n",
    "\n",
    "**1. Gissning:**\n",
    "Perceptronen räknar:\n",
    "`Summa = (1 * 0.5) + (1 * -0.4) + 0.0 = 0.5 - 0.4 = 0.1`\n",
    "Eftersom `0.1 > 0`, blir output `1` (Gå på festen).\n",
    "\n",
    "**2. Misstag:**\n",
    "Gissningen (`1`) stämmer inte med facit (`0`). Du borde ha stannat hemma! Felet är `facit - gissning = 0 - 1 = -1`.\n",
    "\n",
    "**3. Lärdom:**\n",
    "Eftersom felet är negativt, var summan för hög. Perceptronen måste justera sina vikter för att sänka summan nästa gång den ser en liknande situation. Den använder en enkel uppdateringsregel: `ny_vikt_x = gammal_vikt_x + inlärningsfaktor * fel * input_x`. Vi sätter inlärningsfaktorn `α` till `0.1`.\n",
    "\n",
    "- `w1_ny = 0.5 + 0.1 * (-1) * 1 = 0.4` (Vikten för \"vänner\" minskar lite)\n",
    "- `w2_ny = -0.4 + 0.1 * (-1) * 1 = -0.5` (Vikten för \"prov\" blir mer negativ)\n",
    "- `b_ny = 0.0 + 0.1 * (-1) = -0.1` (Biasen justeras också)\n",
    "\n",
    "Med de nya parametrarna (`w1=0.4, w2=-0.5, b=-0.1`), låt oss testa igen:\n",
    "`Summa = (1 * 0.4) + (1 * -0.5) - 0.1 = 0.4 - 0.5 - 0.1 = -0.2`\n",
    "Nu är summan negativ, och outputen blir `0`. Perceptronen har lärt sig!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 2:**\n",
    "Tänk dig en ny situation. Du är en social person vars grundinställning är att man alltid går på fest om man inte har någon information alls.\n",
    "\n",
    "- **Situation:** Du vet inget om festen. Inga vänner har sagt något (`x1=0`) och du har inget prov (`x2=0`).\n",
    "- **Facit:** Din grundinställning säger \"Gå!\" (facit = `1`).\n",
    "\n",
    "Vi använder de senast inlärda vikterna & bias: `w1=0.4, w2=-0.5, b=-0.1`.\n",
    "\n",
    "**1. Gissning:**\n",
    "`Summa = (0 * 0.4) + (0 * -0.5) + (-0.1) = -0.1`\n",
    "Eftersom summan inte är större än 0, blir output `0` (Stanna hemma).\n",
    "\n",
    "**2. Misstag:**\n",
    "Gissningen (`0`) stämmer inte med facit (`1`). Felet är `1 - 0 = 1`.\n",
    "\n",
    "**3. Försök till lärdom:**\n",
    "Perceptronen justerar nu sina vikter & bias för att höja summan.\n",
    "\n",
    "- `w1_ny = 0.4 + 0.1 * (1) * 0 = 0.4` (Ingen ändring!)\n",
    "- `w2_ny = -0.5 + 0.1 * (1) * 0 = -0.5` (Ingen ändring!)\n",
    "- `b_ny = -0.1 + 0.1 * (1) = 0.0` (Biasen justeras)\n",
    "\n",
    "**Här ser vi varför bias är så viktigt!** Biasen är inte beroende av någon input, så den kan alltid justeras om perceptronen beräknat fel svar.\n",
    "\n",
    "Detta visar hur vikter och bias har två olika men lika viktiga jobb, både mekaniskt och geometriskt.\n",
    "\n",
    "- **Vikterna** lär sig mönster baserat på den **input** som finns. **Geometriskt justerar de orienteringen (lutningen) på beslutsgränsen.**\n",
    "- **Biasen** lär sig vad neuronens **standard-svar** ska vara. **Geometriskt frigör den beslutsgränsen från origo, vilket låter den justera sin position för att bäst separera datan.**\n",
    "\n",
    "Utan en justerbar bias är neuronens beslutsgräns permanent fastlåst vid nollpunkten och kan bara rotera, vilket gör den oförmögen att lösa många problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En Perceptron i Python\n",
    "\n",
    "Nu när vi förstår teorin är det dags att omsätta den i praktiken. Öppna filen `perceptron.py` och granska hur `Perceptron`-klassen utökar `Neuron` med en `fit`-metod. Det finns några TODO:s i filen. Implementera koden som beskrivits ovan i denna fil.\n",
    "\n",
    "Låt oss testa perceptronen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slutgiltiga inlärda vikter: [-0.028760136094900884, -0.24831574955026048]\n",
      "Slutgiltig inlärd bias: 0.13551932491901836\n",
      "\n",
      "--- Testar den tränade modellen ---\n",
      "Input: [0, 0] -> Prediktion: 1, Förväntat: 1 ✓\n",
      "Input: [0, 1] -> Prediktion: 0, Förväntat: 0 ✓\n",
      "Input: [1, 0] -> Prediktion: 1, Förväntat: 1 ✓\n",
      "Input: [1, 1] -> Prediktion: 0, Förväntat: 0 ✓\n"
     ]
    }
   ],
   "source": [
    "from perceptron import Perceptron\n",
    "\n",
    "# 1. Definiera vår träningsdata\n",
    "# inputs = [[vänner, prov], ...]\n",
    "inputs = [\n",
    "    [0, 0],  # Inga vänner, inget prov\n",
    "    [0, 1],  # Inga vänner, prov\n",
    "    [1, 0],  # Vänner, inget prov\n",
    "    [1, 1]   # Vänner, prov\n",
    "]\n",
    "\n",
    "# Facit: 1 = Gå, 0 = Stanna hemma\n",
    "targets = [1, 0, 1, 0]\n",
    "\n",
    "# 2. Skapa en instans av vår Perceptron\n",
    "perceptron = Perceptron(num_inputs=2, learning_rate=0.1, n_iterations=5)\n",
    "\n",
    "# 3. Kör träningen\n",
    "perceptron.fit(inputs, targets)\n",
    "\n",
    "# 4. Granska resultatet\n",
    "print(f\"Slutgiltiga inlärda vikter: {perceptron.weights}\")\n",
    "print(f\"Slutgiltig inlärd bias: {perceptron.bias}\")\n",
    "\n",
    "# 5. Testa den färdigtränade modellen\n",
    "print(\"\\n--- Testar den tränade modellen ---\")\n",
    "for inp, target in zip(inputs, targets):\n",
    "    pred = perceptron.predict(inp)\n",
    "    status = \"✓\" if pred == target else \"✗\"\n",
    "    print(f\"Input: {inp} -> Prediktion: {pred}, Förväntat: {target} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analys av resultatet:**\n",
    "\n",
    "När du kör koden kommer du se hur vikterna och biasen justeras för varje iteration. Det slutgiltiga resultatet kommer troligen att visa:\n",
    "\n",
    "- En **negativ vikt** för den andra inputen (`w2`, provet). Detta betyder att ett prov starkt talar _emot_ att gå på fest.\n",
    "- En vikt nära **noll** för den första inputen (`w1`, vänner). Modellen har lärt sig att denna faktor inte är avgörande för beslutet i detta specifika dataset.\n",
    "- En **positiv bias**. Detta representerar en grundinställning att \"gå på fest\" om inga andra starka skäl (som ett prov) finns.\n",
    "\n",
    "Modellen har alltså framgångsrikt lärt sig den logiska regeln från datan!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptronen på ett riktigt dataset (Iris)\n",
    "\n",
    "Nu tar vi steget från ett påhittat problem till ett verkligt. Vi ska använda det berömda **Iris-datasetet**, som innehåller mätningar av tre olika arter av Iris-blommor.\n",
    "\n",
    "En viktig begränsning med vår enkla Perceptron är att den är en **binär klassificerare**, vilket betyder att den bara kan svara på frågor med två möjliga utfall (som 0 eller 1, Ja eller Nej). Iris-datasetet har tre arter, så vi måste förenkla problemet: **\"Är denna blomma en Iris-Setosa, eller inte?\"**\n",
    "\n",
    "Vi kommer också bara att använda två av de fyra tillgängliga måtten (features/inputs) för att hålla det enkelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storlek på träningsdata: 105 exempel\n",
      "Storlek på testdata: 45 exempel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Ladda och förbered datan\n",
    "iris = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# Välj alla rader, och första 2 kolumnerna (sepal length och width)\n",
    "X = iris[[\"sepal_length\",\"sepal_width\"]].values\n",
    "\n",
    "# Skapa facit: 1 om species == \"setosa\", annars 0\n",
    "y = (iris[\"species\"] == \"setosa\").astype(int).values\n",
    "\n",
    "\n",
    "# 2. DELA UPP DATAN\n",
    "# 30% blir testdata, 70% träningsdata\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1337)\n",
    "\n",
    "print(f\"Storlek på träningsdata: {len(X_train)} exempel\")\n",
    "print(f\"Storlek på testdata: {len(X_test)} exempel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test på Iris-datasetet ---\n",
      "Noggrannhet på testdatan: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 3. Skapa och träna en Perceptron-instans PÅ TRÄNINGSDATAN\n",
    "iris_perceptron = Perceptron(num_inputs=2, learning_rate=0.1, n_iterations=10)\n",
    "iris_perceptron.fit(X_train.tolist(), y_train.tolist())\n",
    "\n",
    "# 4. Gör prediktioner och utvärdera PÅ TESTDATAN\n",
    "predictions = [iris_perceptron.predict(inputs) for inputs in X_test.tolist()]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"\\n--- Test på Iris-datasetet ---\")\n",
    "print(f\"Noggrannhet på testdatan: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analys av resultatet:**\n",
    "\n",
    "Du kommer att se att vår enkla Perceptron uppnår mycket imponerande noggrannhet på detta problem! Detta beror på att Iris Setosa-blommorna är så pass olika de andra två arterna (baserat på dessa två features) att det går att dra en perfekt rak linje mellan dem. Detta kallas att datan är **linjärt separerbar**, vilket är det ideala scenariot för en Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Övning 0: Bevisa Perceptronens Gränser\n",
    "\n",
    "**Syfte:** Att praktiskt demonstrera varför en enskild Perceptron inte kan lösa icke-linjära problem som XOR, precis som Minsky och Papert argumenterade.\n",
    "\n",
    "XOR (\"exclusive or\") är ett klassiskt logiskt problem. Outputen är `1` om **exakt en** av inputarna är `1`, annars är outputen `0`.\n",
    "\n",
    "| x1 | x2 | XOR |\n",
    "|----|----|-----|\n",
    "| 0  | 0  | 0   |\n",
    "| 0  | 1  | 1   |\n",
    "| 1  | 0  | 1   |\n",
    "| 1  | 1  | 0   |\n",
    "\n",
    "**Din uppgift:** Träna din Perceptron på XOR-data och observera vad som händer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testar den tränade Perceptronen på XOR-data:\n",
      "Input: [0, 0], Prediktion: 1, Korrekt: 0 ✗\n",
      "Input: [0, 1], Prediktion: 1, Korrekt: 1 ✓\n",
      "Input: [1, 0], Prediktion: 0, Korrekt: 1 ✗\n",
      "Input: [1, 1], Prediktion: 0, Korrekt: 0 ✓\n",
      "\n",
      "Noggrannhet: 2/4\n",
      "Vikter: [-0.3516630615579981, -0.05997168488656354]\n",
      "Bias: 0.1405694912985062\n"
     ]
    }
   ],
   "source": [
    "# Data för XOR\n",
    "X_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_xor = [0, 1, 1, 0]\n",
    "\n",
    "# Skapa och träna din Perceptron med många iterationer\n",
    "xor_perceptron = Perceptron(num_inputs=2, learning_rate=0.1, n_iterations=500)\n",
    "xor_perceptron.fit(X_xor, y_xor)\n",
    "\n",
    "# Testa den tränade modellen\n",
    "print(\"\\nTestar den tränade Perceptronen på XOR-data:\")\n",
    "correct = 0\n",
    "for inp, target in zip(X_xor, y_xor):\n",
    "    prediction = xor_perceptron.predict(inp)\n",
    "    status = \"✓\" if prediction == target else \"✗\"\n",
    "    if prediction == target:\n",
    "        correct += 1\n",
    "    print(f\"Input: {inp}, Prediktion: {prediction}, Korrekt: {target} {status}\")\n",
    "\n",
    "print(f\"\\nNoggrannhet: {correct}/{len(X_xor)}\")\n",
    "print(f\"Vikter: {xor_perceptron.weights}\")\n",
    "print(f\"Bias: {xor_perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Reflektera:** Lyckas din Perceptron lösa XOR-problemet? Varför/varför inte?\n",
    ">\n",
    "> Du har nu praktiskt bevisat Perceptronens begränsning. Den kan inte hitta en rak linje som separerar XOR-datan. Detta motiverar varför vi behöver gå vidare till nästa nivå: nätverk av neuroner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Övning 1: Överlevare från Titanic\n",
    "\n",
    "**Syfte:** Din perceptron beräknar en matematisk formel. Den kraschar om du matar in text (\"male\") eller tomma värden (NaN). Här får du öva på rollen som **Data Engineer**: att städa och forma data så att en maskin kan förstå den. Verkligheten för många AI-uppgifter är att detta kan vara en stor del av jobbet.\n",
    "\n",
    "Målet är binärt: för en given passagerare, gissa om den **överlevde (1) eller dog (0)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Före städning ---\n",
      "   survived  pclass     sex   age     fare\n",
      "0         0       3    male  22.0   7.2500\n",
      "1         1       1  female  38.0  71.2833\n",
      "2         1       3  female  26.0   7.9250\n",
      "3         1       1  female  35.0  53.1000\n",
      "4         0       3    male  35.0   8.0500\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   survived  891 non-null    int64  \n",
      " 1   pclass    891 non-null    int64  \n",
      " 2   sex       891 non-null    object \n",
      " 3   age       714 non-null    float64\n",
      " 4   fare      891 non-null    float64\n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 34.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ladda datasetet\n",
    "titanic = pd.read_csv(\"data/titanic.csv\")\n",
    "\n",
    "# Välj ut relevanta kolumner (Features)\n",
    "# Survived är vårt FACIT (Target)\n",
    "df = titanic[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\"]]\n",
    "\n",
    "print(\"--- Före städning ---\")\n",
    "print(df.head())\n",
    "print(df.info())  # Notera att 'age' saknar många värden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning (städning)\n",
    "\n",
    "Vi måste göra tre saker:\n",
    "1. **Hantera NaN:** Vi kan inte gissa åldern på de som saknar den (just nu). Ta bort rader som saknar data.\n",
    "2. **Text till siffror:** \"male\"/\"female\" måste bli 0/1.\n",
    "3. **Normalisering:** `age` (0-80) och `fare` (0-500) har helt olika skalor. Vi måste klämma in dem mellan 0-1 så att inte biljettpriset \"dränker\" all annan info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Efter städning ---\n",
      "   survived  pclass  sex       age      fare\n",
      "0         0     1.0    0  0.271174  0.014151\n",
      "1         1     0.0    1  0.472229  0.139136\n",
      "2         1     1.0    1  0.321438  0.015469\n",
      "3         1     0.0    1  0.434531  0.103644\n",
      "4         0     1.0    0  0.434531  0.015713\n"
     ]
    }
   ],
   "source": [
    "# 1. Ta bort rader med saknade värden\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Mappa text till siffror\n",
    "# Vi gör om 'sex' så att male=0, female=1\n",
    "df[\"sex\"] = df[\"sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "# 3. Normalisera numeriska värden (Min-Max)\n",
    "df[[\"age\", \"fare\", \"pclass\"]] = scaler.fit_transform(df[[\"age\", \"fare\", \"pclass\"]])\n",
    "\n",
    "print(\"\\n--- Efter städning ---\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Din uppgift\n",
    "\n",
    "Träna din Perceptron på Titanic-datan och beräkna noggrannheten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noggrannhet på Titanic: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Förbered data för träning\n",
    "X = df[[\"pclass\", \"sex\", \"age\", \"fare\"]].values\n",
    "y = df[\"survived\"].values\n",
    "\n",
    "# TODO: Skapa och träna en Perceptron\n",
    "titanic_perceptron = Perceptron(num_inputs=4, learning_rate=0.1, n_iterations=50)\n",
    "titanic_perceptron.fit(X.tolist(), y.tolist())\n",
    "\n",
    "# TODO: Beräkna noggrannheten\n",
    "correct = 0\n",
    "for inp, target in zip(X, y):\n",
    "    answer = titanic_perceptron.predict(inp.tolist())\n",
    "    if round(answer) == target:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(X)\n",
    "print(f\"Noggrannhet på Titanic: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vad har vi lärt oss?\n",
    "\n",
    "I denna notebook har vi:\n",
    "\n",
    "1. Förstått hur en **biologisk neuron** fungerar och hur vi kan modellera den matematiskt\n",
    "2. Byggt en `Neuron`-klass som kan göra prediktioner baserat på vikter och bias\n",
    "3. Utökat neuronen till en `Perceptron` som kan **lära sig** från data\n",
    "4. Testat perceptronen på riktiga dataset (Iris, Titanic)\n",
    "5. **Upptäckt begränsningen:** Perceptronen kan inte lösa XOR!\n",
    "\n",
    "## Nästa steg\n",
    "\n",
    "I nästa notebook ska vi försöka lösa ett nytt problem: **multi-class klassificering**. Vad händer om vi inte bara vill säga \"ja eller nej\", utan klassificera något i flera kategorier?\n",
    "\n",
    "Spoiler: En enda perceptron räcker inte..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
